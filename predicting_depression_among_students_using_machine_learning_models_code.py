# -*- coding: utf-8 -*-
"""Predicting Depression Among Students Using Machine Learning Models - Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TvRLbkHVCTYHd-X9TBL49WyvLO4Ll3e0
"""

#reading dataset from google drive
import pandas as pd
file_link = "https://drive.google.com/file/d/14wnd5i6s2ksmj-rJthlnjBmPLHzchxBH/view?usp=sharing"
file_id = file_link.split("/")[-2]
csv_url = f"https://drive.google.com/uc?id={file_id}"
df = pd.read_csv(csv_url)
df.head(10)

"""# EDA + Preprocessing"""

print(df.columns)
print(df.shape)

df.info()

df.isnull().sum()

"""* only 3 null values are found at the "Financial Stress" Column. these values need to be replaced by the median of this column during preprocessing.
* Irrelevant column : id
"""

# Handle missing values
df['Financial Stress'].fillna(df['Financial Stress'].median(), inplace=True)

# Drop irrelevant columns
df = df.drop(columns=['id'])

df.isnull().sum()

df.describe()

df.duplicated().sum()

df['Depression'].value_counts()

#visual of the target variable
import matplotlib.pyplot as plt

plt.figure(figsize=(5, 3))
df['Depression'].value_counts().plot(kind='bar', color=['lightcoral', 'lightgreen'])
plt.title('Class Distribution of Depression')
plt.xlabel('Depression')
plt.ylabel('Count')
plt.show()

df['Depression'].value_counts(normalize=True) * 100

"""* key finding: An imbalance in dataset is observed ~~~~~ SMOTE"""

#visual of relationship between depression and sleep duration
plt.figure(figsize=(6, 4))
pd.crosstab(df['Sleep Duration'], df['Depression']).plot(kind='bar', color=['lightcoral', 'lightgreen'])
plt.title('Depression by Sleep Duration')
plt.xlabel('Sleep Duration')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

#visual of relationship between depression and mental illness
plt.figure(figsize=(6, 4))
pd.crosstab(df['Family History of Mental Illness'], df['Depression']).plot(kind='bar',color=['lightcoral', 'lightgreen'])
plt.title('Depression by Family History of Mental Illness')
plt.xlabel('Family History of Mental Illness')
plt.ylabel('Count')
plt.show()

#visual of relationship between depression and mental illness
plt.figure(figsize=(6, 4))
pd.crosstab(df['Dietary Habits'], df['Depression']).plot(kind='bar',color=['lightcoral', 'lightgreen'])
plt.title('Depression by Dietary Habits')
plt.xlabel('Dietary Habits')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

#Correlation Heatmap
import numpy as np
import matplotlib.pyplot as plt
num_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()
if "id" in num_cols_all:
    num_cols_all = [c for c in num_cols_all if c.lower() != "id"]
if len(num_cols_all) >= 2:
    corr = df[num_cols_all].corr()
    print("Numeric features correlation")
    display(corr.round(2))

    plt.figure(figsize=(8, 6))
    plt.imshow(corr, interpolation='nearest')
    plt.title("Correlation Heatmap (Numeric Features)")
    plt.colorbar()
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.index)), corr.index)
    plt.tight_layout()
    plt.show()
else:
    print("Not enough numeric columns for correlation.")

"""## Exploring categorical features: Gender, City, Sleep Duration, Degree, Profession, Dietary Habits, Have you ever had suicidal thoughts ?, Family History of Mental Illness

"""

print(df['Profession'].value_counts())

"""since we are working on "Student Depression Prediction & the number of non-student rows is significantly small, dropping all no-student rows & deleting the "profession" column."""

df = df[df['Profession'] == 'Student'].reset_index(drop=True)
# print(df['Profession'].value_counts())

df = df.drop(columns=['Profession'])

df.columns

# categorical features: Gender, City, Sleep Duration, Degree, Dietary Habits, Have you ever had suicidal thoughts ?, Family History of Mental Illness

print(df[['Gender', 'City', 'Sleep Duration', 'Degree', "Dietary Habits", "Have you ever had suicidal thoughts ?", "Family History of Mental Illness"]].head(10))

# unique values in each categorical column
for col in ['Gender', 'City', 'Sleep Duration', 'Degree', "Dietary Habits", "Have you ever had suicidal thoughts ?", "Family History of Mental Illness"]:
    print(f"\nUnique values in {col}:")
    print(df[col].unique())

"""* key finding: The City column has messy data. Apart from real cities, it also contains degree names (M.Tech, ME, M.Com) and personal names (Bhavna, Vaanya) which clearly indicates mislabeled data. - needs fixing"""

# cleaning City:

valid_cities = ['Visakhapatnam', 'Bangalore', 'Srinagar', 'Varanasi', 'Jaipur',
                'Pune', 'Thane', 'Chennai', 'Nagpur', 'Nashik', 'Vadodara',
                'Kalyan', 'Rajkot', 'Ahmedabad', 'Kolkata', 'Mumbai', 'Lucknow',
                'Indore', 'Surat', 'Ludhiana', 'Bhopal', 'Meerut', 'Agra',
                'Ghaziabad', 'Hyderabad', 'Vasai-Virar', 'Kanpur', 'Patna',
                'Faridabad', 'Delhi']

# Replacing invalid cities with 'Other':
df['City'] = df['City'].apply(lambda x: x if x in valid_cities else 'Other')
print(df['City'].unique())
print(df['City'].value_counts())

"""# Handling categorical features:

* Gender = Label Encoding (0/1)

* City = One-Hot Encoding

* Sleep Duration = Ordinal Encoding

* Degree = Ordinal Encoding

* Dietary Habits =  Ordinal Encoding

* Have you ever had suicidal thoughts ? = Label Encoding (0/1)

* Family History of Mental Illness = Label Encoding (0/1)


"""

from sklearn.preprocessing import LabelEncoder

le_gender = LabelEncoder()

df['Gender'] = le_gender.fit_transform(df['Gender'])

print(df['Gender'].head(10))

#City
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop=None, sparse_output=False, handle_unknown='ignore')

city_encoded = encoder.fit_transform(df[['City']])


city_df = pd.DataFrame(city_encoded, columns=encoder.get_feature_names_out(['City']))

df = pd.concat([df, city_df], axis=1)

df.head()

#Sleep Duration
from sklearn.preprocessing import OrdinalEncoder

sleep_order = [['Less than 5 hours', '5-6 hours', '7-8 hours', 'More than 8 hours', 'Others']]

ordinal_encoder = OrdinalEncoder(categories=sleep_order)

df['Sleep Duration Encoded'] = ordinal_encoder.fit_transform(df[['Sleep Duration']])

"""here,
* Less than 5 hours = 0

* 5-6 hours = 1

* 7-8 hours = 2

* More than 8 hours = 3

* Others = 4


"""

df[['Sleep Duration', 'Sleep Duration Encoded']].head(10)

#Degree
degree_mapping = {
    "Class 12": 0,
    "BA": 1, "BSc": 1, "BCA": 1, "B.Ed": 1, "LLB": 1, "BE": 1,
    "BHM": 1, "B.Com": 1, "B.Arch": 1, "B.Tech": 1, "BBA": 1, "B.Pharm": 1,
    "Others": 1,
    "MA": 2, "M.Tech": 2, "M.Ed": 2, "MSc": 2, "M.Pharm": 2,
    "MCA": 2, "MBA": 2, "M.Com": 2, "ME": 2, "MHM": 2, "LLM": 2,
    "MD": 3, "MBBS": 3, "PhD": 3
}

df["Degree_encoded"] = df["Degree"].map(degree_mapping)

print(df[["Degree", "Degree_encoded"]].head(20))

"""here,

* 0 = Class 12

* 1 = Bachelor’s + Others

* 2 = Master’s

* 3 = Doctorate / Professional
"""

#Dietary Habits:

diet_mapping = {
    'Unhealthy': 0,
    'Others': 1,
    'Moderate': 2,
    'Healthy': 3
}

df['Dietary_Habits_encoded'] = df['Dietary Habits'].map(diet_mapping)

df[["Dietary Habits","Dietary_Habits_encoded"]].head()

#Have you ever had suicidal thoughts ?

le_suicide = LabelEncoder()

df['Suicidal_Thoughts'] = le_suicide.fit_transform(df['Have you ever had suicidal thoughts ?'])

df = df.drop(columns=['Have you ever had suicidal thoughts ?'])

df["Suicidal_Thoughts"].head(10)

"""Here,
* yes = 1
* no = 0
"""

#Family History of Mental Illness

le_family = LabelEncoder()

df['Family_History'] = le_family.fit_transform(df['Family History of Mental Illness'])

df = df.drop(columns=['Family History of Mental Illness'])

df['Family_History' ].head(10)

"""here,
* yes = 1
* no = 0

# Applying SMOTE
"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

X = df.drop(columns=['City', 'Sleep Duration', 'Degree', 'Dietary Habits', 'Depression'])
y = df['Depression']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

smote = SMOTE(random_state=42)

X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("Before SMOTE:\n", y_train.value_counts())
print("\nAfter SMOTE:\n", y_train_res.value_counts())

from sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

from scipy.stats import randint, uniform, loguniform
from sklearn.metrics import accuracy_score

"""## **MODELS WITH HYPER PARAMETER TUNING**

## **Logistic Regression**
"""

# Logistic Regression with GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

log_params = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear']
}

log_model = GridSearchCV(
    LogisticRegression(max_iter=1000),
    log_params,
    cv=5,
    scoring='f1'
)

log_model.fit(X_train_res, y_train_res)

print("Best Logistic Regression Params:", log_model.best_params_)
# print("Classification Report:\n", classification_report(y_test, log_model.predict(X_test)))
print("Accuracy:", accuracy_score(y_test, log_model.predict(X_test)))

"""## **Random Forest**"""

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf_model = GridSearchCV(
    RandomForestClassifier(random_state=42),
    rf_params,
    cv=5,
    scoring='f1'
)

rf_model.fit(X_train_res, y_train_res)

print("Best Random Forest Params:", rf_model.best_params_)
print("Accuracy:", accuracy_score(y_test, rf_model.predict(X_test)))
# print("Classification Report:\n", classification_report(y_test, rf_model.predict(X_test)))

"""# **Decision Tree**"""

# Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

dt_params = {
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

dt_model = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    dt_params,
    cv=5,
    scoring='f1'
)

dt_model.fit(X_train_res, y_train_res)

print("Best Decision Tree Params:", dt_model.best_params_)
print("Accuracy:", accuracy_score(y_test, dt_model.predict(X_test)))
# print("Classification Report:\n", classification_report(y_test, dt_model.predict(X_test)))

"""# **KNN**"""

# K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier

knn_params = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

knn_model = GridSearchCV(
    KNeighborsClassifier(),
    knn_params,
    cv=5,
    scoring='f1'
)

knn_model.fit(X_train_res, y_train_res)

print("Best KNN Params:", knn_model.best_params_)
print("Accuracy:", accuracy_score(y_test, knn_model.predict(X_test)))
# print("Classification Report:\n", classification_report(y_test, knn_model.predict(X_test)))

"""# **Adaboost**"""

# AdaBoost Classifier
from sklearn.ensemble import AdaBoostClassifier

ada_params = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1]
}

ada_model = GridSearchCV(
    AdaBoostClassifier(random_state=42),
    ada_params,
    cv=5,
    scoring='f1'
)

ada_model.fit(X_train_res, y_train_res)

print("Best AdaBoost Params:", ada_model.best_params_)
print("Accuracy:", accuracy_score(y_test, ada_model.predict(X_test)))
# print("Classification Report:\n", classification_report(y_test, ada_model.predict(X_test)))

"""# **Gradient Boosting**"""

# Gradient Boosting Classifier
from sklearn.ensemble import GradientBoostingClassifier

gb_params = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5]
}

gb_model = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    gb_params,
    cv=5,
    scoring='f1'
)

gb_model.fit(X_train_res, y_train_res)

print("Best Gradient Boosting Params:", gb_model.best_params_)
print("Accuracy:", accuracy_score(y_test, gb_model.predict(X_test)))
# print("Classification Report:\n", classification_report(y_test, gb_model.predict(X_test)))