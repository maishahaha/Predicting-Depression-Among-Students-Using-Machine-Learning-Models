# -*- coding: utf-8 -*-
"""02_09_22101497_24341267_22101913

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1giNMmh_sdMro50o7DwGvL2H5_6zMh_Te
"""

import pandas as pd

file_link = "https://drive.google.com/file/d/14wnd5i6s2ksmj-rJthlnjBmPLHzchxBH/view?usp=sharing"

file_id = file_link.split("/")[-2]
csv_url = f"https://drive.google.com/uc?id={file_id}"
df = pd.read_csv(csv_url)
df.head(10)

"""# EDA + Preprocessing"""

print(df.columns)
print(df.shape)

df.info()

df.isnull().sum()

"""* only 3 null values are found at the "Financial Stress" Column. these values need to be replaced by the median of this column during preprocessing.
* Irrelevant column : id
"""

# Handle missing values
df['Financial Stress'].fillna(df['Financial Stress'].median(), inplace=True)

# Drop irrelevant columns
df = df.drop(columns=['id'])

df.isnull().sum()

df.describe()

df.duplicated().sum()

df['Depression'].value_counts()

#visual of the target variable
import matplotlib.pyplot as plt

plt.figure(figsize=(5, 3))
df['Depression'].value_counts().plot(kind='bar', color=['lightcoral', 'lightgreen'])
plt.title('Class Distribution of Depression')
plt.xlabel('Depression')
plt.ylabel('Count')
plt.show()

df['Depression'].value_counts(normalize=True) * 100

"""* key finding: An imbalance in dataset is observed ~~~~~ SMOTE"""

#Correlation Heatmap
import numpy as np
import matplotlib.pyplot as plt
num_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()
if "id" in num_cols_all:
    num_cols_all = [c for c in num_cols_all if c.lower() != "id"]
if len(num_cols_all) >= 2:
    corr = df[num_cols_all].corr()
    print("Numeric features correlation")
    display(corr.round(2))

    plt.figure(figsize=(8, 6))
    plt.imshow(corr, interpolation='nearest')
    plt.title("Correlation Heatmap (Numeric Features)")
    plt.colorbar()
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.index)), corr.index)
    plt.tight_layout()
    plt.show()
else:
    print("Not enough numeric columns for correlation.")

"""here, the Correlation heatmap shows no strong feature-target correlations

"""



"""## Exploring categorical features: Gender, City, Sleep Duration, Degree, Profession, Dietary Habits, Have you ever had suicidal thoughts ?, Family History of Mental Illness

"""

print(df['Profession'].value_counts())

"""since we are working on "Student Depression Prediction" & the number of non-student rows is significantly small, dropping all non-student rows & deleting the "profession" column."""

df = df[df['Profession'] == 'Student'].reset_index(drop=True)
# print(df['Profession'].value_counts())

df = df.drop(columns=['Profession'])

df.columns

# Gender, City, Sleep Duration, Degree, Dietary Habits, Have you ever had suicidal thoughts ?, Family History of Mental Illness

print(df[['Gender', 'City', 'Sleep Duration', 'Degree', "Dietary Habits", "Have you ever had suicidal thoughts ?", "Family History of Mental Illness"]].head(10))

# unique values in each categorical column
for col in ['Gender', 'City', 'Sleep Duration', 'Degree', "Dietary Habits", "Have you ever had suicidal thoughts ?", "Family History of Mental Illness"]:
    print(f"\nUnique values in {col}:")
    print(df[col].unique())

"""* key finding: The City column has messy data. Apart from real cities, it also contains degree names (M.Tech, ME, M.Com) and personal names (Bhavna, Vaanya) which clearly indicates mislabeled data. - needs fixing"""

# cleaning City:

valid_cities = ['Visakhapatnam', 'Bangalore', 'Srinagar', 'Varanasi', 'Jaipur',
                'Pune', 'Thane', 'Chennai', 'Nagpur', 'Nashik', 'Vadodara',
                'Kalyan', 'Rajkot', 'Ahmedabad', 'Kolkata', 'Mumbai', 'Lucknow',
                'Indore', 'Surat', 'Ludhiana', 'Bhopal', 'Meerut', 'Agra',
                'Ghaziabad', 'Hyderabad', 'Vasai-Virar', 'Kanpur', 'Patna',
                'Faridabad', 'Delhi']

# Replacing invalid cities with 'Other':
df['City'] = df['City'].apply(lambda x: x if x in valid_cities else 'Other')

print(df['City'].unique())
print(df['City'].value_counts())

"""# Handling categorical features:

* Gender = Label Encoding (0/1)

* City = One-Hot Encoding

* Sleep Duration = Ordinal Encoding

* Degree = Ordinal Encoding

* Dietary Habits =  Ordinal Encoding

* Have you ever had suicidal thoughts ? = Label Encoding (0/1)

* Family History of Mental Illness = Label Encoding (0/1)


"""

from sklearn.preprocessing import LabelEncoder

le_gender = LabelEncoder()

df['Gender'] = le_gender.fit_transform(df['Gender'])

print(df['Gender'].head(10))

#City
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop=None, sparse_output=False, handle_unknown='ignore')

city_encoded = encoder.fit_transform(df[['City']])


city_df = pd.DataFrame(city_encoded, columns=encoder.get_feature_names_out(['City']))

df = pd.concat([df, city_df], axis=1)

df.head()

#Sleep Duration
from sklearn.preprocessing import OrdinalEncoder

sleep_order = [['Less than 5 hours', '5-6 hours', '7-8 hours', 'More than 8 hours', 'Others']]

ordinal_encoder = OrdinalEncoder(categories=sleep_order)

df['Sleep Duration Encoded'] = ordinal_encoder.fit_transform(df[['Sleep Duration']])

"""here,
* Less than 5 hours = 0

* 5-6 hours = 1

* 7-8 hours = 2

* More than 8 hours = 3

* Others = 4


"""

df[['Sleep Duration', 'Sleep Duration Encoded']].head(10)

#Degree
degree_mapping = {
    "Class 12": 0,
    "BA": 1, "BSc": 1, "BCA": 1, "B.Ed": 1, "LLB": 1, "BE": 1,
    "BHM": 1, "B.Com": 1, "B.Arch": 1, "B.Tech": 1, "BBA": 1, "B.Pharm": 1,
    "Others": 1,
    "MA": 2, "M.Tech": 2, "M.Ed": 2, "MSc": 2, "M.Pharm": 2,
    "MCA": 2, "MBA": 2, "M.Com": 2, "ME": 2, "MHM": 2, "LLM": 2,
    "MD": 3, "MBBS": 3, "PhD": 3
}

df["Degree_encoded"] = df["Degree"].map(degree_mapping)

print(df[["Degree", "Degree_encoded"]].head(20))

"""here,

* 0 = Class 12

* 1 = Bachelor’s + Others

* 2 = Master’s

* 3 = Doctorate / Professional
"""

#Dietary Habits:

diet_mapping = {
    'Unhealthy': 0,
    'Others': 1,
    'Moderate': 2,
    'Healthy': 3
}

df['Dietary_Habits_encoded'] = df['Dietary Habits'].map(diet_mapping)

df[["Dietary Habits","Dietary_Habits_encoded"]].head()

#Have you ever had suicidal thoughts ?

le_suicide = LabelEncoder()

df['Suicidal_Thoughts'] = le_suicide.fit_transform(df['Have you ever had suicidal thoughts ?'])

df = df.drop(columns=['Have you ever had suicidal thoughts ?'])

df["Suicidal_Thoughts"].head(10)

"""Here,
* yes = 1
* no = 0
"""

#Family History of Mental Illness

le_family = LabelEncoder()

df['Family_History'] = le_family.fit_transform(df['Family History of Mental Illness'])

df = df.drop(columns=['Family History of Mental Illness'])

df['Family_History' ].head(10)

"""here,
* yes = 1
* no = 0

# Applying SMOTE
"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

X = df.drop(columns=['City', 'Sleep Duration', 'Degree', 'Dietary Habits', 'Depression'])
y = df['Depression']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

smote = SMOTE(random_state=42)

X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("Before SMOTE:\n", y_train.value_counts())
print("\nAfter SMOTE:\n", y_train_res.value_counts())

"""#models to work on :

* Decision Tree
* Random Forest
* AdaBoost
* Logistic Regression
* KNN
* Naive Bayes
* XGBoost

# Decision Tree:
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV

#without tuning:
dt_default = DecisionTreeClassifier(random_state=42)
dt_default.fit(X_train_res, y_train_res)

y_pred_default = dt_default.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred_default))
print("Test F1 Score:", f1_score(y_test, y_pred_default))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_default))
print("Classification Report:\n", classification_report(y_test, y_pred_default))

#With Hyperparameter Tuning:
dt = DecisionTreeClassifier(random_state=42)

param_grid = {
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search_dt = GridSearchCV(
    estimator=dt,
    param_grid=param_grid,
    cv=5,
    scoring='f1',
)

grid_search_dt.fit(X_train_res, y_train_res)

best_dt = grid_search_dt.best_estimator_
y_pred_best = best_dt.predict(X_test)

print("Best Parameters:", grid_search_dt.best_params_)
print("CV Best F1 Score:", grid_search_dt.best_score_)
print("Test Accuracy:", accuracy_score(y_test, y_pred_best))
print("Test F1 Score:", f1_score(y_test, y_pred_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("Classification Report:\n", classification_report(y_test, y_pred_best))

#comparison:

print(f"#Without Tuning:\n Accuracy: {accuracy_score(y_test, y_pred_default):.4f}, F1: {f1_score(y_test, y_pred_default):.4f}")
print(f"#Tuned:\n Accuracy: {accuracy_score(y_test, y_pred_best):.4f}, F1: {f1_score(y_test, y_pred_best):.4f}")



"""# RandomForest:"""

from sklearn.ensemble import RandomForestClassifier

#Without Tuning:
rf_default = RandomForestClassifier(random_state=42)
rf_default.fit(X_train_res, y_train_res)

y_pred_rf_default = rf_default.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred_rf_default))
print("Test F1 Score:", f1_score(y_test, y_pred_rf_default))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf_default))
print("Classification Report:\n", classification_report(y_test, y_pred_rf_default))

#With Hyperparameter Tuning:
rf = RandomForestClassifier(random_state=42)

param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search_rf = GridSearchCV(
    estimator=rf,
    param_grid=param_grid_rf,
    cv=5,
    scoring='f1'
)

grid_search_rf.fit(X_train_res, y_train_res)

best_rf = grid_search_rf.best_estimator_
y_pred_rf_best = best_rf.predict(X_test)

print("Best Parameters:", grid_search_rf.best_params_)
print("CV Best F1 Score:", grid_search_rf.best_score_)
print("Test Accuracy:", accuracy_score(y_test, y_pred_rf_best))
print("Test F1 Score:", f1_score(y_test, y_pred_rf_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf_best))
print("Classification Report:\n", classification_report(y_test, y_pred_rf_best))

#Comparison:

print(f"#Without Tuning:\n Accuracy: {accuracy_score(y_test, y_pred_rf_default):.4f}, F1: {f1_score(y_test, y_pred_rf_default):.4f}")
print(f"#Tuned:\n Accuracy: {accuracy_score(y_test, y_pred_rf_best):.4f}, F1: {f1_score(y_test, y_pred_rf_best):.4f}")



"""# AdaBoost:"""

from sklearn.ensemble import AdaBoostClassifier

#Without Tuning:

adb_default = AdaBoostClassifier(random_state=42)
adb_default.fit(X_train_res, y_train_res)

y_pred_default = adb_default.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred_default))
print("Test F1 Score:", f1_score(y_test, y_pred_default))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_default))
print("Classification Report:\n", classification_report(y_test, y_pred_default))

#With Hyperparameter Tuning:
param_grid_adb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1]
}


grid_search_adb = GridSearchCV(
    estimator=AdaBoostClassifier(random_state=42),
    param_grid=param_grid_adb,
    cv=5,
    scoring='f1'
)

grid_search_adb.fit(X_train_res, y_train_res)

best_adb = grid_search_adb.best_estimator_
y_pred_best = best_adb.predict(X_test)

print("Best Parameters:", grid_search_adb.best_params_)
print("CV Best F1 Score:", grid_search_adb.best_score_)
print("Test Accuracy:", accuracy_score(y_test, y_pred_best))
print("Test F1 Score:", f1_score(y_test, y_pred_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("Classification Report:\n", classification_report(y_test, y_pred_best))

#Comparison:
print(f"#Without Tuning:\n Accuracy: {accuracy_score(y_test, y_pred_default):.4f}, F1: {f1_score(y_test, y_pred_default):.4f}")
print(f"#Tuned:\n Accuracy: {accuracy_score(y_test, y_pred_best):.4f}, F1: {f1_score(y_test, y_pred_best):.4f}")



"""# Logistic Regression:"""

from sklearn.linear_model import LogisticRegression

#Without Tuning:
lr_default = LogisticRegression(random_state=42, max_iter=1000)
lr_default.fit(X_train_res, y_train_res)

y_pred_default = lr_default.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred_default))
print("Test F1 Score:", f1_score(y_test, y_pred_default))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_default))
print("Classification Report:\n", classification_report(y_test, y_pred_default))

#With Hyperparameter Tuning:
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear']
}

grid_search_lr = GridSearchCV(
    estimator=LogisticRegression(random_state=42, max_iter=1000),
    param_grid=param_grid_lr,
    cv=5,
    scoring='f1'
)

grid_search_lr.fit(X_train_res, y_train_res)


best_lr = grid_search_lr.best_estimator_
y_pred_best = best_lr.predict(X_test)

print("Best Parameters:", grid_search_lr.best_params_)
print("CV Best F1 Score:", grid_search_lr.best_score_)
print("Test Accuracy:", accuracy_score(y_test, y_pred_best))
print("Test F1 Score:", f1_score(y_test, y_pred_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("Classification Report:\n", classification_report(y_test, y_pred_best))

# Comparison:
print(f"#Without Tuning:\n Accuracy: {accuracy_score(y_test, y_pred_default):.4f}, F1: {f1_score(y_test, y_pred_default):.4f}")
print(f"#Tuned:\n Accuracy: {accuracy_score(y_test, y_pred_best):.4f}, F1: {f1_score(y_test, y_pred_best):.4f}")



"""# KNN:"""

from sklearn.neighbors import KNeighborsClassifier

#Without Tuning:
knn_default = KNeighborsClassifier()
knn_default.fit(X_train_res, y_train_res)

y_pred_default = knn_default.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred_default))
print("Test F1 Score:", f1_score(y_test, y_pred_default))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_default))
print("Classification Report:\n", classification_report(y_test, y_pred_default))

#With Hyperparameter Tuning:

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

grid_search_knn = GridSearchCV(
    estimator=KNeighborsClassifier(),
    param_grid=param_grid_knn,
    cv=5,
    scoring='f1'
)

grid_search_knn.fit(X_train_res, y_train_res)

# Best model from grid search
best_knn = grid_search_knn.best_estimator_
y_pred_best = best_knn.predict(X_test)

print("Best Parameters:", grid_search_knn.best_params_)
print("CV Best F1 Score:", grid_search_knn.best_score_)
print("Test Accuracy:", accuracy_score(y_test, y_pred_best))
print("Test F1 Score:", f1_score(y_test, y_pred_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("Classification Report:\n", classification_report(y_test, y_pred_best))

# Comparison:
print(f"#Without Tuning:\n Accuracy: {accuracy_score(y_test, y_pred_default):.4f}, F1: {f1_score(y_test, y_pred_default):.4f}")
print(f"#Tuned:\n Accuracy: {accuracy_score(y_test, y_pred_best):.4f}, F1: {f1_score(y_test, y_pred_best):.4f}")



"""# Naive Bayes:"""

from sklearn.naive_bayes import GaussianNB

#Without Tuning:
nb_default = GaussianNB()
nb_default.fit(X_train_res, y_train_res)

y_pred_default = nb_default.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred_default))
print("Test F1 Score:", f1_score(y_test, y_pred_default))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_default))
print("Classification Report:\n", classification_report(y_test, y_pred_default))

#With Hyperparameter Tuning:
param_grid_nb = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
}

grid_search_nb = GridSearchCV(
    estimator=GaussianNB(),
    param_grid=param_grid_nb,
    cv=5,
    scoring='f1',
)

grid_search_nb.fit(X_train_res, y_train_res)

best_nb = grid_search_nb.best_estimator_
y_pred_best = best_nb.predict(X_test)

print("Best Parameters:", grid_search_nb.best_params_)
print("CV Best F1 Score:", grid_search_nb.best_score_)
print("Test Accuracy:", accuracy_score(y_test, y_pred_best))
print("Test F1 Score:", f1_score(y_test, y_pred_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("Classification Report:\n", classification_report(y_test, y_pred_best))

# Comparison:
print(f"#Without Tuning:\n Accuracy: {accuracy_score(y_test, y_pred_default):.4f}, F1: {f1_score(y_test, y_pred_default):.4f}")
print(f"#Tuned:\n Accuracy: {accuracy_score(y_test, y_pred_best):.4f}, F1: {f1_score(y_test, y_pred_best):.4f}")



"""# XGBoost:"""

from xgboost import XGBClassifier

#Without Tuning:
xgb_default = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_default.fit(X_train_res, y_train_res)

y_pred_default = xgb_default.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred_default))
print("Test F1 Score:", f1_score(y_test, y_pred_default))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_default))
print("Classification Report:\n", classification_report(y_test, y_pred_default))

#With Hyperparameter Tuning:
param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

grid_search_xgb = GridSearchCV(
    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    param_grid=param_grid_xgb,
    cv=5,
    scoring='f1',
    n_jobs=-1
)

grid_search_xgb.fit(X_train_res, y_train_res)

best_xgb = grid_search_xgb.best_estimator_
y_pred_best = best_xgb.predict(X_test)

print("Best Parameters:", grid_search_xgb.best_params_)
print("CV Best F1 Score:", grid_search_xgb.best_score_)
print("Test Accuracy:", accuracy_score(y_test, y_pred_best))
print("Test F1 Score:", f1_score(y_test, y_pred_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("Classification Report:\n", classification_report(y_test, y_pred_best))

# Comparison:
print(f"#Without Tuning:\n Accuracy: {accuracy_score(y_test, y_pred_default):.4f}, F1: {f1_score(y_test, y_pred_default):.4f}")
print(f"#Tuned:\n Accuracy: {accuracy_score(y_test, y_pred_best):.4f}, F1: {f1_score(y_test, y_pred_best):.4f}")

